{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPIIGaze Cross Data Generalization\n",
    "\n",
    "This notebook will guide you through the process of recreating the generalization with MPIIGaze.\n",
    "\n",
    "Next, run each following code block one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Downloading\n",
    "Run this code block to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIFaceGaze_normalized.zip\n",
    "!unzip MPIIFaceGaze_normalized.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate\n",
    "Run this code the initialize the models and data loaders for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parses frames out of a video file for the use of testing the images.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import av\n",
    "import mediapipe as mp\n",
    "import math\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from threading import local\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import h5py\n",
    "import torch.utils.data as data\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# This code is converted from https://github.com/CSAILVision/GazeCapture/blob/master/code/faceGridFromFaceRect.m\n",
    "\n",
    "# Given face detection data, generate face grid data.\n",
    "#\n",
    "# Input Parameters:\n",
    "# - frameW/H: The frame in which the detections exist\n",
    "# - gridW/H: The size of the grid (typically same aspect ratio as the\n",
    "#     frame, but much smaller)\n",
    "# - labelFaceX/Y/W/H: The face detection (x and y are 0-based images\n",
    "#     coordinates)\n",
    "# - parameterized: Whether to actually output the grid or just the\n",
    "#     [x y w h] of the 1s square within the gridW x gridH grid.\n",
    "# https://drive.google.com/drive/folders/1ZcYb4eH2jPndS5nkqQFcLHdGNM9dTF5C?usp=sharing\n",
    "# https://drive.google.com/file/d/gpip1UdhuJ_bulreFyGa8CziK4tdwXeHC2PIh/view?usp=sharing\n",
    "\n",
    "def generate_centered_face_grid(gridW, gridH):\n",
    "    labelFaceGrid = np.zeros(gridW * gridH)\n",
    "    grid = np.zeros((gridH, gridW))\n",
    "\n",
    "    xLo = 7\n",
    "    yLo = 7\n",
    "\n",
    "    xHi = 18\n",
    "    yHi = 18\n",
    "\n",
    "    faceLocation = np.ones((yHi - yLo, xHi - xLo))\n",
    "    grid[yLo:yHi, xLo:xHi] = faceLocation\n",
    "\n",
    "    # Flatten the grid.\n",
    "    grid = np.transpose(grid)\n",
    "    labelFaceGrid = grid.flatten()\n",
    "\n",
    "    return labelFaceGrid\n",
    "\n",
    "def faceGridFromFaceRect(frameW, frameH, gridW, gridH, labelFaceX, labelFaceY, labelFaceW, labelFaceH, parameterized):\n",
    "\n",
    "    scaleX = gridW / frameW\n",
    "    scaleY = gridH / frameH\n",
    "\n",
    "    if parameterized:\n",
    "      labelFaceGrid = np.zeros(4)\n",
    "    else:\n",
    "      labelFaceGrid = np.zeros(gridW * gridH)\n",
    "\n",
    "    grid = np.zeros((gridH, gridW))\n",
    "\n",
    "    # Use one-based images coordinates.\n",
    "    xLo = round(labelFaceX * scaleX)\n",
    "    yLo = round(labelFaceY * scaleY)\n",
    "    w = round(labelFaceW * scaleX)\n",
    "    h = round(labelFaceH * scaleY)\n",
    "\n",
    "    if parameterized:\n",
    "        labelFaceGrid = [xLo, yLo, w, h]\n",
    "    else:\n",
    "        xHi = xLo + w\n",
    "        yHi = yLo + h\n",
    "\n",
    "        # Clamp the values in the range.\n",
    "        xLo = int(min(gridW, max(0, xLo)))\n",
    "        xHi = int(min(gridW, max(0, xHi)))\n",
    "        yLo = int(min(gridH, max(0, yLo)))\n",
    "        yHi = int(min(gridH, max(0, yHi)))\n",
    "\n",
    "        faceLocation = np.ones((yHi - yLo, xHi - xLo))\n",
    "        grid[yLo:yHi, xLo:xHi] = faceLocation\n",
    "\n",
    "        # Flatten the grid.\n",
    "        grid = np.transpose(grid)\n",
    "        labelFaceGrid = grid.flatten()\n",
    "\n",
    "    return labelFaceGrid\n",
    "\n",
    "def get_face_grid(face, frameW, frameH, gridSize):\n",
    "    faceX,faceY,faceW,faceH = face\n",
    "\n",
    "    return faceGridFromFaceRect(frameW, frameH, gridSize, gridSize, faceX, faceY, faceW, faceH, True)\n",
    "\n",
    "\n",
    "def crop_to_bounds(img, bounds):\n",
    "    [x, y, w, h] = bounds\n",
    "    cropped = img[y:y + h, x:x + w]\n",
    "    return cropped\n",
    "\n",
    "\n",
    "gridSize = 25\n",
    "\n",
    "def get_frames(video_file, stream=None):\n",
    "    \"\"\"\n",
    "    Parses all frames out of the given video file and returns an array of PIL images.\n",
    "    \"\"\"\n",
    "\n",
    "    container = av.open(video_file)\n",
    "    video = container.streams.video[0]\n",
    "\n",
    "    to_return = []\n",
    "    for idx, frame in enumerate(container.decode(video)):\n",
    "        image = cv2.cvtColor(frame.to_rgb().to_ndarray(), cv2.COLOR_RGB2BGR)\n",
    "        frame_time = float(frame.pts * video.time_base)\n",
    "        if stream is None or not callable(stream):\n",
    "            to_return.append([image, frame_time])\n",
    "        else:\n",
    "            stream(image, frame_time, idx)\n",
    "    container.close()\n",
    "\n",
    "    return to_return\n",
    "\n",
    "detector_storage = local()\n",
    "\n",
    "def detect_features(np_img):\n",
    "    if not hasattr(detector_storage, \"detector\"):\n",
    "        detector_storage.base_options = python.BaseOptions(model_asset_path='detector.tflite')\n",
    "        detector_storage.options = vision.FaceDetectorOptions(base_options=detector_storage.base_options)\n",
    "        detector_storage.detector = vision.FaceDetector.create_from_options(detector_storage.options)\n",
    "    \n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=np_img)\n",
    "    dresult = detector_storage.detector.detect(mp_image)\n",
    "    if len(dresult.detections) == 0:\n",
    "        return [np_img, [], []]\n",
    "\n",
    "    im_width = np_img.shape[1]\n",
    "    im_height = np_img.shape[0]\n",
    "    detection = dresult.detections[0]\n",
    "    right_eye = detection.keypoints[0]\n",
    "    left_eye = detection.keypoints[1]\n",
    "\n",
    "    face_bbx = detection.bounding_box\n",
    "    built_face_bbx = [face_bbx.origin_x, face_bbx.origin_y, face_bbx.width, face_bbx.height]\n",
    "    face_size = face_bbx.width\n",
    "\n",
    "    right_eye_px = [math.floor(right_eye.x * im_width), math.floor(right_eye.y * im_height)]\n",
    "    left_eye_px = [math.floor(left_eye.x * im_width), math.floor(left_eye.y * im_height)]\n",
    "\n",
    "    eye_ratio = math.ceil(face_size / 8)\n",
    "    right_eye_bbx = [right_eye_px[0] - eye_ratio, right_eye_px[1] - eye_ratio, eye_ratio * 2, eye_ratio * 2]\n",
    "    left_eye_bbx = [left_eye_px[0] - eye_ratio, left_eye_px[1] - eye_ratio, eye_ratio * 2, eye_ratio * 2]\n",
    "\n",
    "    return np_img, [built_face_bbx], [[right_eye_bbx, left_eye_bbx],\n",
    "                                      get_face_grid(built_face_bbx, im_width, im_height, 25)]\n",
    "\n",
    "'''\n",
    "Pytorch model for the iTracker.\n",
    "\n",
    "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018. \n",
    "\n",
    "Website: http://gazecapture.csail.mit.edu/\n",
    "\n",
    "Cite:\n",
    "\n",
    "Eye Tracking for Everyone\n",
    "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
    "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
    "\n",
    "@inproceedings{cvpr2016_gazecapture,\n",
    "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
    "Title = {Eye Tracking for Everyone},\n",
    "Year = {2016},\n",
    "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class ItrackerImageModel(nn.Module):\n",
    "    # Used for both eyes (with shared weights) and the face (with unqiue weights)\n",
    "    def __init__(self):\n",
    "        super(ItrackerImageModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.CrossMapLRN2d(size=5, alpha=0.0001, beta=0.75, k=1.0),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.CrossMapLRN2d(size=5, alpha=0.0001, beta=0.75, k=1.0),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 64, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class FaceImageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FaceImageModel, self).__init__()\n",
    "        self.conv = ItrackerImageModel()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12*12*64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class FaceGridModel(nn.Module):\n",
    "    # Model for the face grid pathway\n",
    "    def __init__(self, gridSize = 25):\n",
    "        super(FaceGridModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(gridSize * gridSize, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ITrackerModel(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ITrackerModel, self).__init__()\n",
    "        self.eyeModel = ItrackerImageModel()\n",
    "        self.faceModel = FaceImageModel()\n",
    "        self.gridModel = FaceGridModel()\n",
    "        # Joining both eyes\n",
    "        self.eyesFC = nn.Sequential(\n",
    "            nn.Linear(2*12*12*64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "        # Joining everything\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128+64+128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2),\n",
    "            )\n",
    "\n",
    "    def forward(self, faces, eyesLeft, eyesRight, faceGrids):\n",
    "        # Eye nets\n",
    "        xEyeL = self.eyeModel(eyesLeft)\n",
    "        xEyeR = self.eyeModel(eyesRight)\n",
    "        # Cat and FC\n",
    "        xEyes = torch.cat((xEyeL, xEyeR), 1)\n",
    "        xEyes = self.eyesFC(xEyes)\n",
    "\n",
    "        # Face net\n",
    "        xFace = self.faceModel(faces)\n",
    "        xGrid = self.gridModel(faceGrids)\n",
    "\n",
    "        # Cat all\n",
    "        x = torch.cat((xEyes, xFace, xGrid), 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "'''\n",
    "Data loader for the iTracker.\n",
    "Use prepareDataset.py to convert the dataset from http://gazecapture.csail.mit.edu/ to proper format.\n",
    "\n",
    "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018. \n",
    "\n",
    "Website: http://gazecapture.csail.mit.edu/\n",
    "\n",
    "Cite:\n",
    "\n",
    "Eye Tracking for Everyone\n",
    "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
    "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
    "\n",
    "@inproceedings{cvpr2016_gazecapture,\n",
    "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
    "Title = {Eye Tracking for Everyone},\n",
    "Year = {2016},\n",
    "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "MEAN_PATH = './'\n",
    "\n",
    "def clamp_eyes_to_frame(box, width, height):\n",
    "    if box[0] > 60000:\n",
    "        box[0] = 0\n",
    "    if box[1] > 60000:\n",
    "        box[1] = 0\n",
    "    \n",
    "    if box[0] > 1000:\n",
    "        box[0] = width\n",
    "    if box[1] > 1000:\n",
    "        box[1] = height\n",
    "\n",
    "def loadMetadata(filename, silent = False, struct_as_record = False):\n",
    "    try:\n",
    "        # http://stackoverflow.com/questions/6273634/access-array-contents-from-a-mat-file-loaded-using-scipy-io-loadmat-python\n",
    "        if not silent:\n",
    "            print('\\tReading metadata from %s...' % filename)\n",
    "        metadata = sio.loadmat(filename, squeeze_me=True, struct_as_record=struct_as_record)\n",
    "    except:\n",
    "        print('\\tFailed to read the meta file \"%s\"!' % filename)\n",
    "        return None\n",
    "    return metadata\n",
    "\n",
    "class SubtractMean(object):\n",
    "    \"\"\"Normalize an tensor images with mean.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, meanImg):\n",
    "        self.meanImg = transforms.ToTensor()(meanImg / 255)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor images of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized images.\n",
    "        \"\"\"       \n",
    "        return tensor.sub(self.meanImg)\n",
    "\n",
    "class MPIIGazeData(data.Dataset):\n",
    "    def __init__(self, dataPath: str, split: str = \"all\", imSize=(224,224), gridSize=(25, 25)):\n",
    "        self.dataset_path = dataPath\n",
    "\n",
    "        self.faceMean = loadMetadata(os.path.join(MEAN_PATH, 'mean_face_224.mat'))['image_mean']\n",
    "        self.eyeLeftMean = loadMetadata(os.path.join(MEAN_PATH, 'mean_left_224.mat'))['image_mean']\n",
    "        self.eyeRightMean = loadMetadata(os.path.join(MEAN_PATH, 'mean_right_224.mat'))['image_mean']\n",
    "        self.imSize = imSize\n",
    "        self.gridSize = gridSize\n",
    "        self.screenDistanceCM = 50.8\n",
    "\n",
    "        self.transformFace = transforms.Compose([\n",
    "            transforms.Resize(self.imSize),\n",
    "            transforms.ToTensor(),\n",
    "            SubtractMean(meanImg=self.faceMean),\n",
    "        ])\n",
    "        self.transformEyeL = transforms.Compose([\n",
    "            transforms.Resize(self.imSize),\n",
    "            transforms.ToTensor(),\n",
    "            SubtractMean(meanImg=self.eyeLeftMean),\n",
    "        ])\n",
    "        self.transformEyeR = transforms.Compose([\n",
    "            transforms.Resize(self.imSize),\n",
    "            transforms.ToTensor(),\n",
    "            SubtractMean(meanImg=self.eyeRightMean),\n",
    "        ])\n",
    "\n",
    "        if split == 'train':\n",
    "            subject_split = range(0, 14)\n",
    "        elif split == 'test':\n",
    "            subject_split = range(14, 15)\n",
    "        else:\n",
    "            subject_split = range(0, 15)\n",
    "\n",
    "        self.indices = []\n",
    "        with h5py.File(self.dataset_path, 'r') as f:\n",
    "            for i in subject_split:\n",
    "                count = f.get(f'p{i:02}/count')\n",
    "                for n in range(0, count[()]):\n",
    "                    self.indices.append([i, n])\n",
    "\n",
    "\n",
    "    def transform_angle(self, angle):\n",
    "        pitch = angle[0]\n",
    "        yaw = angle[1]\n",
    "\n",
    "        y_offset = -math.tan(pitch) * self.screenDistanceCM\n",
    "        x_offset = math.tan(yaw) * self.screenDistanceCM\n",
    "\n",
    "        return np.array([x_offset, y_offset], np.float32)\n",
    "\n",
    "\n",
    "    def __getitem__(\n",
    "            self,\n",
    "            index: int):\n",
    "        subject, idx = self.indices[index]\n",
    "        with h5py.File(self.dataset_path, 'r') as f:\n",
    "            image = f.get(f'p{subject:02}/image/{idx:04}')[()]\n",
    "            eyes = f.get(f'p{subject:02}/eyes/{idx:04}')[()]\n",
    "            gaze = f.get(f'p{subject:02}/gaze/{idx:04}')[()]\n",
    "\n",
    "        reye, leye = eyes\n",
    "        clamp_eyes_to_frame(reye, image.shape[1], image.shape[0])\n",
    "        clamp_eyes_to_frame(leye, image.shape[1], image.shape[0])\n",
    "\n",
    "        imFace = image\n",
    "        imEyeR = crop_to_bounds(image, reye)\n",
    "        imEyeL = crop_to_bounds(image, leye)\n",
    "\n",
    "        imFace = Image.fromarray(cv2.cvtColor(imFace, cv2.COLOR_BGR2RGB))\n",
    "        imEyeR = Image.fromarray(cv2.cvtColor(imEyeR, cv2.COLOR_BGR2RGB))\n",
    "        imEyeL = Image.fromarray(cv2.cvtColor(imEyeL, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        imFace = self.transformFace(imFace)\n",
    "        imEyeR = self.transformEyeR(imEyeR)\n",
    "        imEyeL = self.transformEyeL(imEyeL)\n",
    "\n",
    "        face_grid = generate_centered_face_grid(*self.gridSize)\n",
    "        face_grid = torch.FloatTensor(face_grid)\n",
    "        gaze = self.transform_angle(gaze)\n",
    "        gaze = torch.FloatTensor(gaze)\n",
    "        return imFace, imEyeL, imEyeR, face_grid, gaze\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Run this code block to preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pathlib\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "\n",
    "def add_mat_data_to_hdf5(person_id: str, dataset_dir: pathlib.Path,\n",
    "                         output_path: pathlib.Path, sem: threading.Semaphore) -> None:\n",
    "    with h5py.File(dataset_dir / f'{person_id}.mat', 'r') as f_input:\n",
    "        images = f_input.get('Data/data')[()]\n",
    "        labels = f_input.get('Data/label')[()][:, :4]\n",
    "    assert len(images) == len(labels) == 3000\n",
    "\n",
    "    images = images.transpose(0, 2, 3, 1).astype(np.uint8)\n",
    "    \n",
    "    poses = labels[:, 2:]\n",
    "    gazes = labels[:, :2]\n",
    "\n",
    "    filtered_images = []\n",
    "    filtered_poses = []\n",
    "    filtered_gazes = []\n",
    "    eyes = []\n",
    "    for i, image in tqdm.tqdm(enumerate(images), desc=\"Eye processing\"):\n",
    "        im = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        img, face_bbx, eye_features = detect_features(im)\n",
    "        if len(face_bbx) == 0:\n",
    "            continue\n",
    "\n",
    "        right_eye = eye_features[0][0]\n",
    "        left_eye = eye_features[0][1]\n",
    "        \n",
    "        eyes.append([np.array(right_eye).astype(np.uint16), np.array(left_eye).astype(np.uint16)])\n",
    "        filtered_images.append(images[i])\n",
    "        filtered_poses.append(poses[i])\n",
    "        filtered_gazes.append(gazes[i])\n",
    "\n",
    "    sem.acquire()\n",
    "    with h5py.File(output_path, 'a') as f_output:\n",
    "        f_output.create_dataset(f'{person_id}/count', data=len(filtered_images))\n",
    "        for index, (image, gaze, eye_pair,\n",
    "                    pose) in tqdm.tqdm(enumerate(zip(filtered_images, filtered_gazes, eyes, filtered_poses)),\n",
    "                                       leave=False):\n",
    "            f_output.create_dataset(f'{person_id}/image/{index:04}',\n",
    "                                    data=image)\n",
    "            f_output.create_dataset(f'{person_id}/eyes/{index:04}',\n",
    "                                    data=eye_pair)\n",
    "            f_output.create_dataset(f'{person_id}/pose/{index:04}', data=pose)\n",
    "            f_output.create_dataset(f'{person_id}/gaze/{index:04}', data=gaze)\n",
    "    sem.release()\n",
    "\n",
    "\n",
    "def main():\n",
    "    output_dir = 'MPIIGaze'\n",
    "    dataset = 'MPIIFaceGaze_normalizad'\n",
    "\n",
    "    output_dir = pathlib.Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    output_path = output_dir / 'MPIIFaceGaze.h5'\n",
    "    if output_path.exists():\n",
    "        raise ValueError(f'{output_path} already exists.')\n",
    "\n",
    "    dataset_dir = pathlib.Path(dataset)\n",
    "    sem = threading.Semaphore()\n",
    "    all_futures = []\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for person_id in range(15):\n",
    "            person_id = f'p{person_id:02}'\n",
    "            future = executor.submit(add_mat_data_to_hdf5, person_id, dataset_dir, output_path, sem)\n",
    "            all_futures.append(future)\n",
    "        \n",
    "        with tqdm.tqdm(total=15) as pbar:\n",
    "            def update(thing):\n",
    "                nonlocal pbar\n",
    "                pbar.update(1)\n",
    "                return\n",
    "            future.add_done_callback(update)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Run this code block to re-train the algorithm and get the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently achieves 4.5059 (2.12) on the validation set.\n",
    "# Currently achieves 14.8343 (3.85) on MPIIGaze validation.\n",
    "# Currently achieves 8.9463 (2.99) on true MPIIGaze cross generalization\n",
    "\n",
    "import shutil, os, time, argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n",
    "'''\n",
    "Train/test code for iTracker.\n",
    "\n",
    "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018. \n",
    "\n",
    "Website: http://gazecapture.csail.mit.edu/\n",
    "\n",
    "Cite:\n",
    "\n",
    "Eye Tracking for Everyone\n",
    "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
    "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
    "\n",
    "@inproceedings{cvpr2016_gazecapture,\n",
    "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
    "Title = {Eye Tracking for Everyone},\n",
    "Year = {2016},\n",
    "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "# Change there flags to control what happens.\n",
    "doLoad = False # Load checkpoint at the beginning\n",
    "doTest = False\n",
    "data_path = 'MPIIGaze/MPIIFaceGaze.h5'\n",
    "\n",
    "device = 'cuda'\n",
    "cpu_workers = 2\n",
    "cuda_workers = 8\n",
    "epochs = 25\n",
    "batch_size = 64 if device == 'cpu' else torch.cuda.device_count() * 50\n",
    "\n",
    "base_lr = 0.0001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "print_freq = 10\n",
    "prec1 = 0\n",
    "best_prec1 = 1e20\n",
    "lr = base_lr\n",
    "\n",
    "count_test = 0\n",
    "count = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    global args, best_prec1, weight_decay, momentum\n",
    "\n",
    "    model = ITrackerModel()\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.to(torch.device(device))\n",
    "    imSize=(224,224)\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.deterministic = False   \n",
    "\n",
    "    epoch = 0\n",
    "    if doLoad:\n",
    "        saved = load_checkpoint()\n",
    "        if saved:\n",
    "            print('Loading checkpoint for epoch %05d with loss %.5f (which is the mean squared error not the actual linear error)...' % (saved['epoch'], saved['best_prec1']))\n",
    "            state = saved['state_dict']\n",
    "            try:\n",
    "                model.module.load_state_dict(state)\n",
    "            except:\n",
    "                model.load_state_dict(state)\n",
    "            epoch = saved['epoch']\n",
    "            best_prec1 = saved['best_prec1']\n",
    "        else:\n",
    "            print('Warning: Could not read checkpoint!')\n",
    "\n",
    "    dataTrain = MPIIGazeData(dataPath = data_path, split='train', imSize = imSize)\n",
    "    dataVal = MPIIGazeData(dataPath = data_path, split='test', imSize = imSize)\n",
    "   \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataTrain,\n",
    "        batch_size=batch_size, shuffle=True,\n",
    "        num_workers=cpu_workers if device == 'cpu' else cuda_workers, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataVal,\n",
    "        batch_size=batch_size, shuffle=False,\n",
    "        num_workers=cpu_workers if device == 'cpu' else cuda_workers, pin_memory=True)\n",
    "\n",
    "    criterion = nn.MSELoss().to(torch.device(device)) # Mean squared error loss function\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr, # Uses stochastic gradient descent\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "    # Quick test\n",
    "    if doTest:\n",
    "        validate(val_loader, model, criterion, epoch)\n",
    "        return\n",
    "\n",
    "    for epoch in range(0, epoch):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "    for epoch in range(epoch, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion, epoch)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion,optimizer, epoch):\n",
    "    global count\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    dev = torch.device(device)\n",
    "\n",
    "    for i, (imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        imFace = imFace.to(dev)\n",
    "        imEyeL = imEyeL.to(dev)\n",
    "        imEyeR = imEyeR.to(dev)\n",
    "        faceGrid = faceGrid.to(dev)\n",
    "        gaze = gaze.to(dev)\n",
    "\n",
    "        # compute output\n",
    "        output = model(imFace, imEyeL, imEyeR, faceGrid)\n",
    "\n",
    "        loss = criterion(output, gaze)\n",
    "\n",
    "        losses.update(loss.data.item(), imFace.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "        print('Epoch (train): [{0}][{1}/{2}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "            epoch, i, len(train_loader), batch_time=batch_time,\n",
    "            data_time=data_time, loss=losses))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    global count_test\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    lossesLin = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for i, (imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        imFace = imFace.to(device=device)\n",
    "        imEyeL = imEyeL.to(device=device)\n",
    "        imEyeR = imEyeR.to(device=device)\n",
    "        faceGrid = faceGrid.to(device=device)\n",
    "        gaze = gaze.to(device=device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(imFace, imEyeL, imEyeR, faceGrid)\n",
    "\n",
    "        loss = criterion(output, gaze)\n",
    "\n",
    "        lossLin = output - gaze\n",
    "        lossLin = torch.mul(lossLin, lossLin)\n",
    "        lossLin = torch.sum(lossLin, 1)\n",
    "        lossLin = torch.mean(torch.sqrt(lossLin))\n",
    "\n",
    "        losses.update(loss.data.item(), imFace.size(0))\n",
    "        lossesLin.update(lossLin.item(), imFace.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print('Epoch (val): [{0}][{1}/{2}]\\t'\n",
    "                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                'Error L2 {lossLin.val:.4f} ({lossLin.avg:.4f})\\t'.format(\n",
    "            epoch, i, len(val_loader), batch_time=batch_time,\n",
    "            loss=losses, lossLin=lossesLin))\n",
    "\n",
    "    return lossesLin.avg\n",
    "\n",
    "CHECKPOINTS_PATH = '.'\n",
    "\n",
    "\n",
    "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
    "    filename = os.path.join(CHECKPOINTS_PATH, filename)\n",
    "    print(filename)\n",
    "    if not os.path.isfile(filename):\n",
    "        return None\n",
    "    state = torch.load(filename)\n",
    "    return state\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    if not os.path.isdir(CHECKPOINTS_PATH):\n",
    "        os.makedirs(CHECKPOINTS_PATH, 0o777)\n",
    "    bestFilename = os.path.join(CHECKPOINTS_PATH, 'best_' + filename)\n",
    "    filename = os.path.join(CHECKPOINTS_PATH, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, bestFilename)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = base_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.state_dict()['param_groups']:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print('DONE')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
