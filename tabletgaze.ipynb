{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabletGaze Cross Data Generalization\n",
    "\n",
    "This notebook will guide you through the process of recreating the generalization with TabletGaze.\n",
    "First, you should obtain the dataset and extract its contents to a folder named `TabletGazeDataset`.\n",
    "\n",
    "Next, run each following code block one by one.\n",
    "\n",
    "Note:\n",
    "Due to the amount of time to train the algorithm, we weren't able to retrain the model on the TabletGaze dataset. Instead, we took the pre-built model (that was provided with the source code) and ran validation on it. Through this, we obtained a RMSE of 2.12 (representing centimeters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate\n",
    "Run this code to register all boilerplate code for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parses frames out of a video file for the use of testing the images.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import av\n",
    "import mediapipe as mp\n",
    "import math\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from threading import local\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import h5py\n",
    "import torch.utils.data as data\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# This code is converted from https://github.com/CSAILVision/GazeCapture/blob/master/code/faceGridFromFaceRect.m\n",
    "\n",
    "# Given face detection data, generate face grid data.\n",
    "#\n",
    "# Input Parameters:\n",
    "# - frameW/H: The frame in which the detections exist\n",
    "# - gridW/H: The size of the grid (typically same aspect ratio as the\n",
    "#     frame, but much smaller)\n",
    "# - labelFaceX/Y/W/H: The face detection (x and y are 0-based images\n",
    "#     coordinates)\n",
    "# - parameterized: Whether to actually output the grid or just the\n",
    "#     [x y w h] of the 1s square within the gridW x gridH grid.\n",
    "# https://drive.google.com/drive/folders/1ZcYb4eH2jPndS5nkqQFcLHdGNM9dTF5C?usp=sharing\n",
    "# https://drive.google.com/file/d/gpip1UdhuJ_bulreFyGa8CziK4tdwXeHC2PIh/view?usp=sharing\n",
    "\n",
    "def generate_centered_face_grid(gridW, gridH):\n",
    "    labelFaceGrid = np.zeros(gridW * gridH)\n",
    "    grid = np.zeros((gridH, gridW))\n",
    "\n",
    "    xLo = 7\n",
    "    yLo = 7\n",
    "\n",
    "    xHi = 18\n",
    "    yHi = 18\n",
    "\n",
    "    faceLocation = np.ones((yHi - yLo, xHi - xLo))\n",
    "    grid[yLo:yHi, xLo:xHi] = faceLocation\n",
    "\n",
    "    # Flatten the grid.\n",
    "    grid = np.transpose(grid)\n",
    "    labelFaceGrid = grid.flatten()\n",
    "\n",
    "    return labelFaceGrid\n",
    "\n",
    "def faceGridFromFaceRect(frameW, frameH, gridW, gridH, labelFaceX, labelFaceY, labelFaceW, labelFaceH, parameterized):\n",
    "\n",
    "    scaleX = gridW / frameW\n",
    "    scaleY = gridH / frameH\n",
    "\n",
    "    if parameterized:\n",
    "      labelFaceGrid = np.zeros(4)\n",
    "    else:\n",
    "      labelFaceGrid = np.zeros(gridW * gridH)\n",
    "\n",
    "    grid = np.zeros((gridH, gridW))\n",
    "\n",
    "    # Use one-based images coordinates.\n",
    "    xLo = round(labelFaceX * scaleX)\n",
    "    yLo = round(labelFaceY * scaleY)\n",
    "    w = round(labelFaceW * scaleX)\n",
    "    h = round(labelFaceH * scaleY)\n",
    "\n",
    "    if parameterized:\n",
    "        labelFaceGrid = [xLo, yLo, w, h]\n",
    "    else:\n",
    "        xHi = xLo + w\n",
    "        yHi = yLo + h\n",
    "\n",
    "        # Clamp the values in the range.\n",
    "        xLo = int(min(gridW, max(0, xLo)))\n",
    "        xHi = int(min(gridW, max(0, xHi)))\n",
    "        yLo = int(min(gridH, max(0, yLo)))\n",
    "        yHi = int(min(gridH, max(0, yHi)))\n",
    "\n",
    "        faceLocation = np.ones((yHi - yLo, xHi - xLo))\n",
    "        grid[yLo:yHi, xLo:xHi] = faceLocation\n",
    "\n",
    "        # Flatten the grid.\n",
    "        grid = np.transpose(grid)\n",
    "        labelFaceGrid = grid.flatten()\n",
    "\n",
    "    return labelFaceGrid\n",
    "\n",
    "def get_face_grid(face, frameW, frameH, gridSize):\n",
    "    faceX,faceY,faceW,faceH = face\n",
    "\n",
    "    return faceGridFromFaceRect(frameW, frameH, gridSize, gridSize, faceX, faceY, faceW, faceH, True)\n",
    "\n",
    "\n",
    "def crop_to_bounds(img, bounds):\n",
    "    [x, y, w, h] = bounds\n",
    "    cropped = img[y:y + h, x:x + w]\n",
    "    return cropped\n",
    "\n",
    "\n",
    "gridSize = 25\n",
    "\n",
    "def get_frames(video_file, stream=None):\n",
    "    \"\"\"\n",
    "    Parses all frames out of the given video file and returns an array of PIL images.\n",
    "    \"\"\"\n",
    "\n",
    "    container = av.open(video_file)\n",
    "    video = container.streams.video[0]\n",
    "\n",
    "    to_return = []\n",
    "    for idx, frame in enumerate(container.decode(video)):\n",
    "        image = cv2.cvtColor(frame.to_rgb().to_ndarray(), cv2.COLOR_RGB2BGR)\n",
    "        frame_time = float(frame.pts * video.time_base)\n",
    "        if stream is None or not callable(stream):\n",
    "            to_return.append([image, frame_time])\n",
    "        else:\n",
    "            stream(image, frame_time, idx)\n",
    "    container.close()\n",
    "\n",
    "    return to_return\n",
    "\n",
    "detector_storage = local()\n",
    "\n",
    "def detect_features(np_img):\n",
    "    if not hasattr(detector_storage, \"detector\"):\n",
    "        detector_storage.base_options = python.BaseOptions(model_asset_path='detector.tflite')\n",
    "        detector_storage.options = vision.FaceDetectorOptions(base_options=detector_storage.base_options)\n",
    "        detector_storage.detector = vision.FaceDetector.create_from_options(detector_storage.options)\n",
    "    \n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=np_img)\n",
    "    dresult = detector_storage.detector.detect(mp_image)\n",
    "    if len(dresult.detections) == 0:\n",
    "        return [np_img, [], []]\n",
    "\n",
    "    im_width = np_img.shape[1]\n",
    "    im_height = np_img.shape[0]\n",
    "    detection = dresult.detections[0]\n",
    "    right_eye = detection.keypoints[0]\n",
    "    left_eye = detection.keypoints[1]\n",
    "\n",
    "    face_bbx = detection.bounding_box\n",
    "    built_face_bbx = [face_bbx.origin_x, face_bbx.origin_y, face_bbx.width, face_bbx.height]\n",
    "    face_size = face_bbx.width\n",
    "\n",
    "    right_eye_px = [math.floor(right_eye.x * im_width), math.floor(right_eye.y * im_height)]\n",
    "    left_eye_px = [math.floor(left_eye.x * im_width), math.floor(left_eye.y * im_height)]\n",
    "\n",
    "    eye_ratio = math.ceil(face_size / 8)\n",
    "    right_eye_bbx = [right_eye_px[0] - eye_ratio, right_eye_px[1] - eye_ratio, eye_ratio * 2, eye_ratio * 2]\n",
    "    left_eye_bbx = [left_eye_px[0] - eye_ratio, left_eye_px[1] - eye_ratio, eye_ratio * 2, eye_ratio * 2]\n",
    "\n",
    "    return np_img, [built_face_bbx], [[right_eye_bbx, left_eye_bbx],\n",
    "                                      get_face_grid(built_face_bbx, im_width, im_height, 25)]\n",
    "\n",
    "'''\n",
    "Pytorch model for the iTracker.\n",
    "\n",
    "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018. \n",
    "\n",
    "Website: http://gazecapture.csail.mit.edu/\n",
    "\n",
    "Cite:\n",
    "\n",
    "Eye Tracking for Everyone\n",
    "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
    "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
    "\n",
    "@inproceedings{cvpr2016_gazecapture,\n",
    "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
    "Title = {Eye Tracking for Everyone},\n",
    "Year = {2016},\n",
    "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class ItrackerImageModel(nn.Module):\n",
    "    # Used for both eyes (with shared weights) and the face (with unqiue weights)\n",
    "    def __init__(self):\n",
    "        super(ItrackerImageModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.CrossMapLRN2d(size=5, alpha=0.0001, beta=0.75, k=1.0),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.CrossMapLRN2d(size=5, alpha=0.0001, beta=0.75, k=1.0),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 64, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class FaceImageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FaceImageModel, self).__init__()\n",
    "        self.conv = ItrackerImageModel()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12*12*64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class FaceGridModel(nn.Module):\n",
    "    # Model for the face grid pathway\n",
    "    def __init__(self, gridSize = 25):\n",
    "        super(FaceGridModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(gridSize * gridSize, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ITrackerModel(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ITrackerModel, self).__init__()\n",
    "        self.eyeModel = ItrackerImageModel()\n",
    "        self.faceModel = FaceImageModel()\n",
    "        self.gridModel = FaceGridModel()\n",
    "        # Joining both eyes\n",
    "        self.eyesFC = nn.Sequential(\n",
    "            nn.Linear(2*12*12*64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "        # Joining everything\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128+64+128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2),\n",
    "            )\n",
    "\n",
    "    def forward(self, faces, eyesLeft, eyesRight, faceGrids):\n",
    "        # Eye nets\n",
    "        xEyeL = self.eyeModel(eyesLeft)\n",
    "        xEyeR = self.eyeModel(eyesRight)\n",
    "        # Cat and FC\n",
    "        xEyes = torch.cat((xEyeL, xEyeR), 1)\n",
    "        xEyes = self.eyesFC(xEyes)\n",
    "\n",
    "        # Face net\n",
    "        xFace = self.faceModel(faces)\n",
    "        xGrid = self.gridModel(faceGrids)\n",
    "\n",
    "        # Cat all\n",
    "        x = torch.cat((xEyes, xFace, xGrid), 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "'''\n",
    "Data loader for the iTracker.\n",
    "Use prepareDataset.py to convert the dataset from http://gazecapture.csail.mit.edu/ to proper format.\n",
    "\n",
    "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018. \n",
    "\n",
    "Website: http://gazecapture.csail.mit.edu/\n",
    "\n",
    "Cite:\n",
    "\n",
    "Eye Tracking for Everyone\n",
    "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
    "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
    "\n",
    "@inproceedings{cvpr2016_gazecapture,\n",
    "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
    "Title = {Eye Tracking for Everyone},\n",
    "Year = {2016},\n",
    "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "MEAN_PATH = './'\n",
    "\n",
    "def clamp_eyes_to_frame(box, width, height):\n",
    "    if box[0] > 60000:\n",
    "        box[0] = 0\n",
    "    if box[1] > 60000:\n",
    "        box[1] = 0\n",
    "    \n",
    "    if box[0] > 1000:\n",
    "        box[0] = width\n",
    "    if box[1] > 1000:\n",
    "        box[1] = height\n",
    "\n",
    "def loadMetadata(filename, silent = False, struct_as_record = False):\n",
    "    try:\n",
    "        # http://stackoverflow.com/questions/6273634/access-array-contents-from-a-mat-file-loaded-using-scipy-io-loadmat-python\n",
    "        if not silent:\n",
    "            print('\\tReading metadata from %s...' % filename)\n",
    "        metadata = sio.loadmat(filename, squeeze_me=True, struct_as_record=struct_as_record)\n",
    "    except:\n",
    "        print('\\tFailed to read the meta file \"%s\"!' % filename)\n",
    "        return None\n",
    "    return metadata\n",
    "\n",
    "class SubtractMean(object):\n",
    "    \"\"\"Normalize an tensor images with mean.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, meanImg):\n",
    "        self.meanImg = transforms.ToTensor()(meanImg / 255)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor images of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized images.\n",
    "        \"\"\"       \n",
    "        return tensor.sub(self.meanImg)\n",
    "\n",
    "class TabletGazePreprocessData(data.Dataset):\n",
    "    def __init__(self, dataPath, split='all', imSize=(224, 224), gridSize=(25, 25)):\n",
    "        \"\"\"\n",
    "        The tablet gaze dataset has extra metadata for each video...\n",
    "\n",
    "        The first, stored in gazePts.mat, stores the order and location of the dot appearance for ALL trials.\n",
    "        The second, stored in startTime.mat, stores the time (in seconds)? that the trial begins. (Discard all frames prior to this timestamp)\n",
    "\n",
    "        The trial was conducted with 35 dot positions. After the start time, every 3 seconds the dot changes positions.\n",
    "        \"\"\"\n",
    "        self.data_path = dataPath\n",
    "        self.imSize = imSize\n",
    "        self.gridSize = gridSize\n",
    "\n",
    "        self.gaze_pts = loadMetadata(os.path.join(dataPath, 'gazePts.mat'), struct_as_record=True)['gazePts'].tolist()\n",
    "        self.gaze_points_x = self.gaze_pts[2]\n",
    "        self.gaze_points_y = self.gaze_pts[3]\n",
    "        self.startTime = loadMetadata(os.path.join(dataPath, 'startTime.mat'), struct_as_record=True)['startTime'].tolist()\n",
    "\n",
    "        self.subjects = 51\n",
    "        self.screenWidthCM = 22.62\n",
    "        self.screenHeightCM = 14.14\n",
    "\n",
    "        self.screenCenterXCM = self.screenWidthCM / 2\n",
    "        self.screenCenterYCM = self.screenHeightCM / 2\n",
    "\n",
    "        if split == 'train':\n",
    "            subject_split = range(0, math.ceil(self.subjects * 0.8)) # Use 80% of the set for training\n",
    "        elif split == 'test':\n",
    "            subject_split = range(math.ceil(self.subjects * 0.8), self.subjects) # Use 20% of the set for testing\n",
    "        else:\n",
    "            subject_split = range(0, self.subjects)\n",
    "\n",
    "        self.indices = []\n",
    "        for subject in subject_split:\n",
    "            for trial in range(0, 4):\n",
    "                for pose in range(0, 4):\n",
    "                    self.indices.append([subject + 1, trial + 1, pose + 1])\n",
    "\n",
    "\n",
    "    def makeGrid(self, params):\n",
    "        gridLen = self.gridSize[0] * self.gridSize[1]\n",
    "        grid = np.zeros([gridLen, ], np.float32)\n",
    "\n",
    "        indsY = np.array([i // self.gridSize[0] for i in range(gridLen)])\n",
    "        indsX = np.array([i % self.gridSize[0] for i in range(gridLen)])\n",
    "        condX = np.logical_and(indsX >= params[0], indsX < params[0] + params[2])\n",
    "        condY = np.logical_and(indsY >= params[1], indsY < params[1] + params[3])\n",
    "        cond = np.logical_and(condX, condY)\n",
    "\n",
    "        grid[cond] = 1\n",
    "        return grid\n",
    "\n",
    "    def get_dot_index(self, subject, trial, pose, current_time):\n",
    "        subject_set = self.startTime[subject]\n",
    "        time_start = subject_set[trial][pose]\n",
    "        # If the trial hasn't started yet, don't process it!\n",
    "        if current_time < time_start:\n",
    "            return -1\n",
    "\n",
    "        # There is a new dot every 3 seconds.\n",
    "        dot_index = math.floor((current_time - time_start) / 3)\n",
    "        return dot_index\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Gets a batch of frames from the given video index.\n",
    "        \"\"\"\n",
    "        subject, trial, pose = self.indices[index]\n",
    "        data_points = []\n",
    "        video_name = f'{subject}/{subject}_{trial}_{pose}.mp4'\n",
    "        path_to_video = os.path.join(self.data_path, video_name)\n",
    "        print('Loading frames for %s' % path_to_video)\n",
    "        num = 0\n",
    "        frame_count = 0\n",
    "\n",
    "        if not os.path.exists(os.path.join(self.data_path, f\"{subject}_{trial}_{pose}_image\")):\n",
    "            os.makedirs(os.path.join(self.data_path, f\"{subject}_{trial}_{pose}_image\"))\n",
    "        def parse_frame(frame, frame_time, frame_idx):\n",
    "            nonlocal num, frame_count\n",
    "            num += 1\n",
    "            if num % 1000 == 0:\n",
    "                print('Loading frame %s for video file %s' % (num, path_to_video))\n",
    "\n",
    "            # Get the dot index and check if the trial hasn't started or has already finished.\n",
    "            dot_index = self.get_dot_index(subject - 1, trial - 1, pose - 1, frame_time)\n",
    "            if dot_index < 0 or dot_index >= 35:\n",
    "                return\n",
    "\n",
    "            features = detect_features(frame)\n",
    "\n",
    "            faces = features[1]\n",
    "            # Skip the face if it wasn't detected!\n",
    "            if len(faces) == 0:\n",
    "                return\n",
    "\n",
    "            imFace = crop_to_bounds(frame, faces[0])\n",
    "            right_eye, left_eye = features[2][0]\n",
    "\n",
    "            face_x = faces[0][0]\n",
    "            face_y = faces[0][1]\n",
    "\n",
    "            # Offset eyes by this amount\n",
    "            right_eye[0] -= face_x\n",
    "            right_eye[1] -= face_y\n",
    "\n",
    "            left_eye[0] -= face_x\n",
    "            left_eye[1] -= face_y\n",
    "\n",
    "            faceGrid = self.makeGrid(features[2][1])\n",
    "            cv2.imwrite(os.path.join(self.data_path, f\"{subject}_{trial}_{pose}_image/{frame_count}.jpg\"), imFace)\n",
    "\n",
    "            # to tensor\n",
    "            data_points.append(\n",
    "                [np.array(faces[0]), np.array(left_eye), np.array(right_eye), faceGrid, frame_time, num - 1])\n",
    "            frame_count += 1\n",
    "\n",
    "        get_frames(path_to_video, parse_frame)\n",
    "        return [data_points, subject, trial, pose]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "class TabletGazePostprocessData(data.Dataset):\n",
    "    def __init__(self, dataPath, preprocess_path='preprocessed.h5', split='all', imSize=(224, 224), gridSize=(25, 25)):\n",
    "        \"\"\"\n",
    "        The tablet gaze dataset has extra metadata for each video...\n",
    "\n",
    "        The first, stored in gazePts.mat, stores the order and location of the dot appearance for ALL trials.\n",
    "        The second, stored in startTime.mat, stores the time (in seconds)? that the trial begins. (Discard all frames prior to this timestamp)\n",
    "\n",
    "        The trial was conducted with 35 dot positions. After the start time, every 3 seconds the dot changes positions.\n",
    "        \"\"\"\n",
    "        self.data_path = dataPath\n",
    "        self.imSize = imSize\n",
    "        self.gridSize = gridSize\n",
    "\n",
    "        self.gaze_pts = loadMetadata(os.path.join(dataPath, 'gazePts.mat'), struct_as_record=True)['gazePts'].tolist()\n",
    "        self.gaze_points_x = self.gaze_pts[2]\n",
    "        self.gaze_points_y = self.gaze_pts[3]\n",
    "        self.startTime = loadMetadata(os.path.join(dataPath, 'startTime.mat'), struct_as_record=True)['startTime'].tolist()\n",
    "\n",
    "        self.faceMean = loadMetadata(os.path.join(MEAN_PATH, 'mean_face_224.mat'))['image_mean']\n",
    "        self.eyeLeftMean = loadMetadata(os.path.join(MEAN_PATH, 'mean_left_224.mat'))['image_mean']\n",
    "        self.eyeRightMean = loadMetadata(os.path.join(MEAN_PATH, 'mean_right_224.mat'))['image_mean']\n",
    "\n",
    "        self.transformFace = transforms.Compose([\n",
    "            transforms.Resize(self.imSize),\n",
    "            transforms.ToTensor(),\n",
    "            SubtractMean(meanImg=self.faceMean),\n",
    "        ])\n",
    "        self.transformEyeL = transforms.Compose([\n",
    "            transforms.Resize(self.imSize),\n",
    "            transforms.ToTensor(),\n",
    "            SubtractMean(meanImg=self.eyeLeftMean),\n",
    "        ])\n",
    "        self.transformEyeR = transforms.Compose([\n",
    "            transforms.Resize(self.imSize),\n",
    "            transforms.ToTensor(),\n",
    "            SubtractMean(meanImg=self.eyeRightMean),\n",
    "        ])\n",
    "\n",
    "        self.subjects = 5 #51\n",
    "        self.preprocess_path = preprocess_path\n",
    "        self.screenWidthCM = 22.62\n",
    "        self.screenHeightCM = 14.14\n",
    "\n",
    "        self.screenCenterXCM = self.screenWidthCM / 2\n",
    "        self.screenCenterYCM = self.screenHeightCM / 2\n",
    "\n",
    "        if split == 'train':\n",
    "            subject_split = range(0, math.ceil(self.subjects * 0.8)) # Use 80% of the set for training\n",
    "        elif split == 'test':\n",
    "            subject_split = range(math.ceil(self.subjects * 0.8), self.subjects) # Use 20% of the set for testing\n",
    "        else:\n",
    "            subject_split = range(0, self.subjects)\n",
    "\n",
    "        self.indices = []\n",
    "        self.h5file = h5py.File(self.preprocess_path, 'r')\n",
    "        for subject in subject_split:\n",
    "            for trial in range(0, 1):\n",
    "                for pose in range(0, 4):\n",
    "                    for file in os.listdir(os.path.join(self.data_path, f\"{subject + 1}_{trial + 1}_{pose + 1}_image\")):\n",
    "                        if \".jpg\" not in file:\n",
    "                            continue\n",
    "\n",
    "                        self.indices.append([subject + 1, trial + 1, pose + 1, int(file.replace(\".jpg\", \"\"))])\n",
    "\n",
    "\n",
    "    def makeGrid(self, params):\n",
    "        gridLen = self.gridSize[0] * self.gridSize[1]\n",
    "        grid = np.zeros([gridLen, ], np.float32)\n",
    "\n",
    "        indsY = np.array([i // self.gridSize[0] for i in range(gridLen)])\n",
    "        indsX = np.array([i % self.gridSize[0] for i in range(gridLen)])\n",
    "        condX = np.logical_and(indsX >= params[0], indsX < params[0] + params[2])\n",
    "        condY = np.logical_and(indsY >= params[1], indsY < params[1] + params[3])\n",
    "        cond = np.logical_and(condX, condY)\n",
    "\n",
    "        grid[cond] = 1\n",
    "        return grid\n",
    "\n",
    "    def get_dot_index(self, subject, trial, pose, current_time):\n",
    "        subject_set = self.startTime[subject]\n",
    "        time_start = subject_set[trial][pose]\n",
    "        # If the trial hasn't started yet, don't process it!\n",
    "        if current_time < time_start:\n",
    "            return -1\n",
    "\n",
    "        # There is a new dot every 3 seconds.\n",
    "        dot_index = math.floor((current_time - time_start) / 3)\n",
    "        return dot_index\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Gets a batch of frames from the given video index.\n",
    "        \"\"\"\n",
    "        subject, trial, pose, frame_num = self.indices[index]\n",
    "        file_name = f\"{subject}_{trial}_{pose}_image/{frame_num}.jpg\"\n",
    "\n",
    "        eyes_l = self.h5file.get(f'{subject}/{trial}_{pose}/eyes_l')[()]\n",
    "        eyes_r = self.h5file.get(f'{subject}/{trial}_{pose}/eyes_r')[()]\n",
    "        metadata_loaded = self.h5file.get(f'{subject}/{trial}_{pose}/metadata')[()]\n",
    "        frametime_loaded = self.h5file.get(f'{subject}/{trial}_{pose}/frame_times')[()]\n",
    "        frame_indices = self.h5file.get(f'{subject}/{trial}_{pose}/frame_indices')[()]\n",
    "\n",
    "        if frame_num >= len(frame_indices):\n",
    "            return\n",
    "\n",
    "        face_grid = metadata_loaded[frame_num]\n",
    "        frame_time = frametime_loaded[frame_num]\n",
    "        leye = eyes_l[frame_num]\n",
    "        reye = eyes_r[frame_num]\n",
    "\n",
    "        # Get the dot index and check if the trial hasn't started or has already finished.\n",
    "        dot_index = self.get_dot_index(subject - 1, trial - 1, pose - 1, frame_time)\n",
    "        if dot_index < 0 or dot_index >= 35:\n",
    "            return\n",
    "\n",
    "        topleft_dot_x_cm = self.gaze_points_x[dot_index]\n",
    "        topleft_dot_y_cm = self.gaze_points_y[dot_index]\n",
    "\n",
    "        # We need to transform the dot locations to be based from the center of the screen, which is what iTracker uses.\n",
    "        dot_x_cm = topleft_dot_x_cm - self.screenCenterXCM\n",
    "        dot_y_cm = -topleft_dot_y_cm\n",
    "        frame = cv2.imread(os.path.join(self.data_path, file_name))\n",
    "\n",
    "        gaze = np.array([dot_x_cm, dot_y_cm], np.float32)\n",
    "        clamp_eyes_to_frame(leye, frame.shape[1], frame.shape[0])\n",
    "        clamp_eyes_to_frame(reye, frame.shape[1], frame.shape[0])\n",
    "\n",
    "        imFace = frame\n",
    "        imEyeR = crop_to_bounds(frame, reye)\n",
    "        imEyeL = crop_to_bounds(frame, leye)\n",
    "\n",
    "        imFace = Image.fromarray(cv2.cvtColor(imFace, cv2.COLOR_BGR2RGB))\n",
    "        imEyeR = Image.fromarray(cv2.cvtColor(imEyeR, cv2.COLOR_BGR2RGB))\n",
    "        imEyeL = Image.fromarray(cv2.cvtColor(imEyeL, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        imFace = self.transformFace(imFace)\n",
    "        imEyeR = self.transformEyeR(imEyeR)\n",
    "        imEyeL = self.transformEyeL(imEyeL)\n",
    "\n",
    "        face_grid = torch.FloatTensor(face_grid)\n",
    "        gaze = torch.FloatTensor(gaze)\n",
    "\n",
    "        # print(\"Yes. We're still running.\")\n",
    "        return [imFace, imEyeL, imEyeR, face_grid, gaze]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Run this code to preprocess and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/NewRaidData/atb00010_cs460/GazeCapture7\n",
      "\tReading metadata from TabletGazeDataset/gazePts.mat...\n",
      "\tReading metadata from TabletGazeDataset/startTime.mat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/816 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading frames for TabletGazeDataset/1/1_1_2.mp4Loading frames for TabletGazeDataset/1/1_1_1.mp4Loading frames for TabletGazeDataset/1/1_1_3.mp4Loading frames for TabletGazeDataset/1/1_1_4.mp4Loading frames for TabletGazeDataset/1/1_2_1.mp4Loading frames for TabletGazeDataset/1/1_2_2.mp4Loading frames for TabletGazeDataset/1/1_2_4.mp4Loading frames for TabletGazeDataset/1/1_3_1.mp4\n",
      "\n",
      "Loading frames for TabletGazeDataset/1/1_3_2.mp4Loading frames for TabletGazeDataset/1/1_3_3.mp4\n",
      "\n",
      "\n",
      "Loading frames for TabletGazeDataset/1/1_3_4.mp4\n",
      "Loading frames for TabletGazeDataset/1/1_4_1.mp4\n",
      "Loading frames for TabletGazeDataset/1/1_4_2.mp4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Loading frames for TabletGazeDataset/1/1_4_3.mp4\n",
      "Loading frames for TabletGazeDataset/1/1_4_4.mp4Loading frames for TabletGazeDataset/2/2_1_1.mp4\n",
      "Loading frames for TabletGazeDataset/2/2_1_2.mp4\n",
      "\n",
      "Loading frames for TabletGazeDataset/2/2_1_3.mp4\n",
      "Loading frames for TabletGazeDataset/2/2_1_4.mp4\n",
      "Loading frames for TabletGazeDataset/2/2_2_1.mp4Loading frames for TabletGazeDataset/2/2_2_2.mp4\n",
      "Loading frames for TabletGazeDataset/2/2_2_3.mp4\n",
      "\n",
      "\n",
      "Loading frames for TabletGazeDataset/2/2_2_4.mp4\n",
      "Loading frames for TabletGazeDataset/1/1_2_3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.064839  889005 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.071634  888725 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.071959  889045 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.083600  888845 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.096350  888525 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.168699  888565 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.173947  888926 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1702223905.327162  890873 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "I0000 00:00:1702223905.327494  890872 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "I0000 00:00:1702223905.337291  890874 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "I0000 00:00:1702223905.343173  890875 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.381065  888805 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1702223905.431002  890934 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1702223905.440605  890932 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "I0000 00:00:1702223905.444513  890933 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.553014  888885 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.646426  888592 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1702223905.649343  890978 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223905.813008  888485 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1702223905.819636  890996 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1702223905.843926  890997 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1702223906.036965  891027 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.103196  888685 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.308551  889205 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.331685  888765 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.406976  888925 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1702223906.414966  891043 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1702223906.536985  891061 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1702223906.556894  891062 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.587098  889285 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1702223906.602158  891091 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.639605  889125 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.684506  888645 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1702223906.763080  891109 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.786410  889126 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.800278  889325 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1702223906.863778  891124 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.883266  889245 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223906.908258  888445 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1702223906.930596  891141 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1702223907.059796  891158 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223907.063782  889365 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1702223907.192974  891174 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1702223907.207497  891175 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1702223907.279756  891204 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1702223907.322507  891206 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702223909.238740  889085 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1702223909.499952  891237 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.104.05), renderer: NVIDIA TITAN RTX/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "  0%|          | 0/816 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m             output\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpose\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/frame_times\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mframe_times_encode)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mpreprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTabletGazeDataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m, in \u001b[0;36mpreprocess_dataset\u001b[0;34m(dataset_path, output_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(TabletGazePreprocessData(dataset_path), pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# if os.path.exists(output_path):\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     os.remove(output_path)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_data \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(loader):\n\u001b[1;32m     22\u001b[0m     face \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m     eye_l \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading frame 1000 for video file TabletGazeDataset/1/1_2_1.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/2/2_2_1.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/2/2_2_2.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/2/2_1_3.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_3_1.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/2/2_1_1.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_4_4.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_4_1.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/2/2_2_3.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/2/2_1_4.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_1_2.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_1_1.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_1_3.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_3_4.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_2_4.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_3_2.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_4_2.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_2_2.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/2/2_1_2.mp4\n",
      "Loading frame 1000 for video file TabletGazeDataset/1/1_2_3.mp4\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset_path, output_path='preprocessed.h5'):\n",
    "    \"\"\"\n",
    "    Produces a .h5 file with annotated data.\n",
    "    \"\"\"\n",
    "    print(os.getcwd())\n",
    "\n",
    "    # Create the data loader and load all frames.\n",
    "    loader = torch.utils.data.DataLoader(TabletGazePreprocessData(dataset_path), pin_memory=True, shuffle=False, num_workers=24)\n",
    "\n",
    "    # if os.path.exists(output_path):\n",
    "    #     os.remove(output_path)\n",
    "\n",
    "    for video_data in tqdm.tqdm(loader):\n",
    "        face = []\n",
    "        eye_l = []\n",
    "        eye_r = []\n",
    "        metadata_record = []\n",
    "        frame_times = []\n",
    "        frame_indices = []\n",
    "        frame_group = video_data[0]\n",
    "        subject = video_data[1].item()\n",
    "        trial = video_data[2].item()\n",
    "        pose = video_data[3].item()\n",
    "        for frame_data in frame_group:\n",
    "            imFace, imEyeL, imEyeR, faceGrid, frame_time, index = frame_data\n",
    "            face.append(imFace.numpy().copy()[0])\n",
    "            eye_l.append(imEyeL.numpy().copy()[0])\n",
    "            eye_r.append(imEyeR.numpy().copy()[0])\n",
    "            frame_times.append(frame_time[0].item())\n",
    "            metadata_record.append(faceGrid.numpy().copy()[0])\n",
    "            frame_indices.append(index.numpy().copy()[0])\n",
    "\n",
    "        face_encode = np.array(face).astype(np.uint16)\n",
    "        eye_l_encode = np.array(eye_l).astype(np.uint16)\n",
    "        eye_r_encode = np.array(eye_r).astype(np.uint16)\n",
    "        metadata_encode = np.array(metadata_record).astype(np.float32)\n",
    "        frame_indices_encode = np.array(frame_indices).astype(np.uint32)\n",
    "        frame_times_encode = np.array(frame_times).astype(np.float32)\n",
    "        with h5py.File(output_path, 'a') as output:\n",
    "            output.create_dataset(f'{subject}/{trial}_{pose}/faces', data=face_encode)\n",
    "            output.create_dataset(f'{subject}/{trial}_{pose}/eyes_l', data=eye_l_encode)\n",
    "            output.create_dataset(f'{subject}/{trial}_{pose}/eyes_r', data=eye_r_encode)\n",
    "            output.create_dataset(f'{subject}/{trial}_{pose}/metadata', data=metadata_encode)\n",
    "            output.create_dataset(f'{subject}/{trial}_{pose}/frame_indices', data=frame_indices_encode)\n",
    "            output.create_dataset(f'{subject}/{trial}_{pose}/frame_times', data=frame_times_encode)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    preprocess_dataset('TabletGazeDataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "Run this code to reproduce the validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently achieves 4.5059 (2.12) on the validation set.\n",
    "# Currently achieves 14.8343 (3.85) on MPIIGaze validation.\n",
    "# Currently achieves 8.9463 (2.99) on true MPIIGaze cross generalization\n",
    "\n",
    "import shutil, os, time, argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n",
    "'''\n",
    "Train/test code for iTracker.\n",
    "\n",
    "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018. \n",
    "\n",
    "Website: http://gazecapture.csail.mit.edu/\n",
    "\n",
    "Cite:\n",
    "\n",
    "Eye Tracking for Everyone\n",
    "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
    "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
    "\n",
    "@inproceedings{cvpr2016_gazecapture,\n",
    "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
    "Title = {Eye Tracking for Everyone},\n",
    "Year = {2016},\n",
    "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "# Change there flags to control what happens.\n",
    "doLoad = True # Load checkpoint at the beginning\n",
    "doTest = True\n",
    "data_path = 'TabletGazeDataset'\n",
    "\n",
    "device = 'cuda'\n",
    "cpu_workers = 2\n",
    "cuda_workers = 8\n",
    "epochs = 25\n",
    "batch_size = 64 if device == 'cpu' else torch.cuda.device_count() * 50\n",
    "\n",
    "base_lr = 0.0001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "print_freq = 10\n",
    "prec1 = 0\n",
    "best_prec1 = 1e20\n",
    "lr = base_lr\n",
    "\n",
    "count_test = 0\n",
    "count = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    global args, best_prec1, weight_decay, momentum\n",
    "\n",
    "    model = ITrackerModel()\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.to(torch.device(device))\n",
    "    imSize=(224,224)\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.deterministic = False   \n",
    "\n",
    "    epoch = 0\n",
    "    if doLoad:\n",
    "        saved = load_checkpoint()\n",
    "        if saved:\n",
    "            print('Loading checkpoint for epoch %05d with loss %.5f (which is the mean squared error not the actual linear error)...' % (saved['epoch'], saved['best_prec1']))\n",
    "            state = saved['state_dict']\n",
    "            try:\n",
    "                model.module.load_state_dict(state)\n",
    "            except:\n",
    "                model.load_state_dict(state)\n",
    "            epoch = saved['epoch']\n",
    "            best_prec1 = saved['best_prec1']\n",
    "        else:\n",
    "            print('Warning: Could not read checkpoint!')\n",
    "\n",
    "    dataTrain = TabletGazePostprocessData(dataPath = data_path, split='train', imSize = imSize)\n",
    "    dataVal = TabletGazePostprocessData(dataPath = data_path, split='test', imSize = imSize)\n",
    "   \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataTrain,\n",
    "        batch_size=batch_size, shuffle=True,\n",
    "        num_workers=cpu_workers if device == 'cpu' else cuda_workers, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataVal,\n",
    "        batch_size=batch_size, shuffle=False,\n",
    "        num_workers=cpu_workers if device == 'cpu' else cuda_workers, pin_memory=True)\n",
    "\n",
    "    criterion = nn.MSELoss().to(torch.device(device)) # Mean squared error loss function\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr, # Uses stochastic gradient descent\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "    # Quick test\n",
    "    if doTest:\n",
    "        validate(val_loader, model, criterion, epoch)\n",
    "        return\n",
    "\n",
    "    for epoch in range(0, epoch):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "    for epoch in range(epoch, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion, epoch)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion,optimizer, epoch):\n",
    "    global count\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    dev = torch.device(device)\n",
    "\n",
    "    for i, (imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        imFace = imFace.to(dev)\n",
    "        imEyeL = imEyeL.to(dev)\n",
    "        imEyeR = imEyeR.to(dev)\n",
    "        faceGrid = faceGrid.to(dev)\n",
    "        gaze = gaze.to(dev)\n",
    "\n",
    "        # compute output\n",
    "        output = model(imFace, imEyeL, imEyeR, faceGrid)\n",
    "\n",
    "        loss = criterion(output, gaze)\n",
    "\n",
    "        losses.update(loss.data.item(), imFace.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "        print('Epoch (train): [{0}][{1}/{2}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "            epoch, i, len(train_loader), batch_time=batch_time,\n",
    "            data_time=data_time, loss=losses))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    global count_test\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    lossesLin = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    for i, (imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        imFace = imFace.to(device=device)\n",
    "        imEyeL = imEyeL.to(device=device)\n",
    "        imEyeR = imEyeR.to(device=device)\n",
    "        faceGrid = faceGrid.to(device=device)\n",
    "        gaze = gaze.to(device=device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(imFace, imEyeL, imEyeR, faceGrid)\n",
    "\n",
    "        loss = criterion(output, gaze)\n",
    "\n",
    "        lossLin = output - gaze\n",
    "        lossLin = torch.mul(lossLin, lossLin)\n",
    "        lossLin = torch.sum(lossLin, 1)\n",
    "        lossLin = torch.mean(torch.sqrt(lossLin))\n",
    "\n",
    "        losses.update(loss.data.item(), imFace.size(0))\n",
    "        lossesLin.update(lossLin.item(), imFace.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print('Epoch (val): [{0}][{1}/{2}]\\t'\n",
    "                'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                'Error L2 {lossLin.val:.4f} ({lossLin.avg:.4f})\\t'.format(\n",
    "            epoch, i, len(val_loader), batch_time=batch_time,\n",
    "            loss=losses, lossLin=lossesLin))\n",
    "\n",
    "    return lossesLin.avg\n",
    "\n",
    "CHECKPOINTS_PATH = '.'\n",
    "\n",
    "\n",
    "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
    "    filename = os.path.join(CHECKPOINTS_PATH, filename)\n",
    "    print(filename)\n",
    "    if not os.path.isfile(filename):\n",
    "        return None\n",
    "    state = torch.load(filename)\n",
    "    return state\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    if not os.path.isdir(CHECKPOINTS_PATH):\n",
    "        os.makedirs(CHECKPOINTS_PATH, 0o777)\n",
    "    bestFilename = os.path.join(CHECKPOINTS_PATH, 'best_' + filename)\n",
    "    filename = os.path.join(CHECKPOINTS_PATH, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, bestFilename)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = base_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.state_dict()['param_groups']:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print('DONE')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
